# Handoff

Handoff is a library that allows you to bootstrap a server that runs scheduled and manually triggered e2e tests written in Go and is extensible through plugins.

## Why Handoff?

More and more companies are building their software as distributed systems.

These are notoriously hard to test end-to-end as the number of services grow. At some point starting all of them and their dependencies locally on the developer's machine becomes unfeasible.

This means that if you want to make sure your system works as expected you need to run tests in an environment that is as close to production as possible, such as a development or staging cluster.

This is where Handoff comes in. You can run Handoff alongside your system as a standalone server that runs all your end-to-end tests either

* on demand (via cli, api or ui) or
* repeatedly through a configurable schedule

On top of that there is a baked in web ui where you can look up the test results and manually trigger new runs.

Other valuable (future/planned) features:

* Matching and linking test runs to logs / traces / metrics generated by the
systems under test (SUT) for easier debugging.
* Running smoke tests on new deployments for automatic rollbacks if necessary (argocd, flux).
* Fighting flaky tests via
 * auto detection
 * triggering multiple scheduled runs of the same test suite to provoke a test failure which can be debugged thereafter
* Maintaining historic test run data for deeper insights on the
stability of the platform and making it available via a `/metrics` endpoint.
* Triggering of alerts on test failures (e.g. via pagerduty).
* Automated notifications on test runs (e.g. via slack messages).
* Github Integration: adding test run results to relevant PRs after they were merged and deployed.
*

## Example

Bootstrapping a server is simple, all you need to do is run this code:

```go
package main

func main() {
	h := handoff.New()
	h.Run()
}
```

To pass in test suites and scheduled runs you can do that by passing in `handoff.WithTestSuite` and `handoff.WithScheduledRun` options to `handoff.New()`.

Another way is to register them via `handoff.Register` before calling `handoff.New()`. This is especially convenient when you want to have your tests in the same repository as the system under test (SUT), which means they would be in a different repository (unless you have a monorepo). In this case the test package could register the tests in an init function like so:

```go
func init() {
    handoff.Register(ts, scheduledRuns)
}
```

and then all the handoff server needs to do is import the test package with a blank identifier:

```go
import _ "github.com/my-org/my-service/tests"
```

For examples see [./cmd/example-server-bootstrap/main.go] and [./internal/packagetestexample].

## Build
```sh
templ generate
go build ./cmd/example-server-bootstrap/
./example-server-bootstrap
```

## Run

```sh
./example-server-bootstrap
```

## Web

If your server is running you can open your browser to e.g. `http://localhost:1337/suites`.

This will show you all available test suites.

To create a new test run you can use the post request in `./requests.http`

```sh
httpyac requests.http
```

## Local dev cluster

Prerequisites:
* Running Docker
* Tilt + Kind installed and on the path

Run

```sh
kind create cluster --config=kind-config.yaml
tilt up
```

If you press `<space>` the tilt UI will open up.

Once handoff is green in the dashboard you should be able to open up the ui here: http://localhost:1337/.

## Test best practices

* Pass in the test context for longer running operations and check if it was cancelled.
* Only log messages via t.Log/t.Logf as other log messages will not show up in the test logs.
* Make sure that code in `setup` is idempotent as it can run more than once.

## Planned features

- [ ] (Feature) Write a tool "transformcli" that uses go:generate and go/ast to transform handoff tests and suites to standard go tests (suite -> test with subtests + init and cleanup)
- [ ] (Feature) Users
  - [ ] Teams (assigned to test suites)
  - [ ] Favorite test suites (UI)
  - [ ] authentication providers (LDAP, Oauth2, ...)
- [ ] (Feature) Test Suite Metadata (desription in markdown, links to e.g. github, documentation, handbook, ...) shown in UI
- [ ] (Feature) Automatic test run retries/backoff on failures
- [ ] (Feature) Add a limit to scheduled runs (stop after X runs or X mins)
- [ ] (Feature) CLI program to run tests and wait for results, export results to json (via the server's http api)
- [ ] (Feature) Dashboard UI that shows handoff statistics, running tests, resource usage (cpu, memory, active go routines...) etc
- [ ] (Feature) Opt-in test timeouts through t.Context and / or providing wrapped handoff functions ( e.g. http clients) to be used in tests  that implement test timeouts
- [ ] (Feature) Add test-suite labels
- [ ] (Feature) If test was run within the context of a PR maybe we can figure out who the author was (via commit emails) and send an email if a test failed.
- [ ] (Feature) Allow running of handoff as headless/cli mode (without http server) that returns a code != 0 if a test has failed (e.g. in github actions CI)
- [ ] (Feature) Add an option to the helm chart to support remote debugging through dlv
- [ ] (Feature) Image for [helm chart](https://helm.sh/docs/topics/chart_tests/) tests for automated helm release rollbacks
- [ ] (Feature) Test suite namespaces for grouping
- [ ] (Feature) Asynchronous plugin hooks with callbacks for slow operations (e.g. http calls)
- [ ] (Plugin) Pagerduty - triger alerts/incidents on failed e2e tests
- [ ] (Plugin) Slack - send messages to slack channels when tests pass / fail
- [ ] (Plugin) Github - pr status checks
- [ ] (Plugin) Jira - add test run results to a PR.
- [ ] (Plugin) Prometheus / Loki / Tempo / ELK stack - find and fetch logs/traces/metrics that are created by tests (e.g. for easier debugging) - e.g. via correlation ids
- [ ] (Technical) Idempotency in post requests via a key to avoid duplicate test runs
- [x] (Technical) Server configuration through either ENV vars or cli flags
- [x] (Technical) Continue test runs on service restart
- [x] (Technical) Graceful server shutdown
- [x] (Technical) Well tested
- [x] (Technical) Registering of `TestSuite`s and `ScheduledRun`s via imported packages
- [x] (Technical) Persistence layer
- [x] (Feature) Persist compressed test logs to save space
- [x] (Feature) Soft test fails that don't fail the entire testsuite. This can be used to help with the chicken/egg problem when you add new tests that target a new service version that is not deployed yet.
- [x] (Feature) Basic webui bundled in the service that shows test run results
- [x] (Feature) Configurable test run retention policy (TTL)
- [x] (Feature) Start test runs via POST requests
- [x] (Feature) Write test suites with multiple tests written in Go
- [x] (Feature) Manual retrying of failed tests
- [x] (Feature) Skip individual tests by calling t.Skip() within a test
- [x] (Feature) Scheduled / recurring test runs (e.g. for soak tests)
- [x] (Feature) Skip test subsets via regex filters passed into a test run
- [x] (Feature) Flaky test detection + metric label
- [x] (Feature) Support existing assertion libraries like stretch/testify
- [x] (Feature) Prometheus /metrics endpoint that exposes test metrics
- [x] (Feature) Basic support for plugins to hook into the test lifecycle

## Potential features

- [ ] (Technical) Websocket that streams test results (like test logs) - this could be used by the cli tool to get live updates on running tests
- [ ] (Technical) Authenticated HTTP requests through TLS client certificates
- [ ] (Feature) Grafana service dashboard template
- [ ] (Feature) Service dashboards that show information of services k8s resources running in a cluster and their test suite runs
- [ ] (Feature) Output go test json report
- [ ] (Feature) Support running tests in languages other than go
- [ ] (Feature) k8s operator / CRDs to configure test runs & schedules (we probably don't need this)

## Open questions

- How to add test timeouts (it's impossible to externally stop goroutines running user provided functions)?

## Non goals

- Implement a new assertion library. We aim to be compatible with existing ones.

## Metrics

Metrics are exposed via the `/metrics` endpoint.

| Name                             | Type    | Description                                 | Labels                        |
| -------------------------------- | ------- | ------------------------------------------- | ----------------------------- |
| handoff_testsuites_running       | gauge   | The number of test suites currently running | namespace, suite_name         |
| handoff_testsuites_started_total | counter | The number of test suite runs started       | namespace, suite_name, result |
| handoff_tests_run_total          | counter | The number of tests run                     | namespace, suite_name, result |
